\begin{thebibliography}{10}

\bibitem{adagrad}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em The Journal of Machine Learning Research}, 12:2121 -- 2159,
  2011.

\bibitem{difficulty}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em AISTATS}, 2010.

\bibitem{goodfellow13}
Ian~J. Goodfellow, David Warde{-}Farley, Mehdi Mirza, Aaron~C. Courville, and
  Yoshua Bengio.
\newblock Maxout networks.
\newblock In {\em Proceedings of the 30th International Conference on Machine
  Learning, {ICML}}, pages 1319--1327, 2013.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock {\em arXiv preprint arXiv:1502.01852}, 2015.

\bibitem{batch_norm}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em arXiv}, 2015.

\bibitem{Adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock {\em CoRR}, abs/1412.6980, 2014.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Computer Science Department, University of Toronto, Tech. Rep},
  1(4):7, 2009.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{Kronecker}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In {\em ICML}, 2015.

\bibitem{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In {\em NIPS workshop on deep learning and unsupervised feature
  learning}, 2011.

\bibitem{neyshabur15b}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock {\em International Conference on Learning Representations (ICLR)
  workshop track}, 2015.

\bibitem{neyshabur15}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock {\em COLT}, 2015.

\bibitem{srebro05}
Nathan Srebro and Adi Shraibman.
\newblock Rank, trace-norm and max-norm.
\newblock In {\em Learning Theory}, pages 545--560. Springer, 2005.

\bibitem{srebro11}
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari.
\newblock On the universality of online mirror descent.
\newblock In {\em Advances in neural information processing systems}, pages
  2645--2653, 2011.

\bibitem{srivastava14}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em The Journal of Machine Learning Research}, 15(1):1929--1958,
  2014.

\bibitem{icml2013}
I.~Sutskever, J.~Martens, George Dahl, and Geoffery Hinton.
\newblock On the importance of momentum and initialization in deep learning.
\newblock In {\em ICML}, 2013.

\end{thebibliography}
