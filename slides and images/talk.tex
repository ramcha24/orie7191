\documentclass[presentation,xcolor={usenames,dvipsnames},10pt]{beamer}

% figures
\newcommand{\home}{.}
\newcommand{\figures}{\home/figures}
\input formatting.tex
\input defs.tex
\usepackage{hyperref}
\hypersetup{colorlinks=true}

\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}
\newcommand{\Diag}{\text{diag}}
%\newcommand{\minimize}[1]{\underset{{#1}}{\text{minimize}}}
\newcommand{\tTheta}{\widetilde{\Theta}}
\newcommand{\numinputs}{d}
\newcommand{\numoutputs}{k}
\newcommand{\numhiddens}{H}
\newcommand{\vu}{\boldsymbol{u}}
\newcommand{\vv}{\boldsymbol{v}}
\newcommand{\vx}{\boldsymbol{x}}
\newcommand{\vy}{\boldsymbol{y}}
\newcommand{\mU}{\boldsymbol{U}}
\newcommand{\mV}{\boldsymbol{V}}
\newcommand{\mW}{\boldsymbol{W}}
\newcommand{\rect}{\text{rect}}
\newcommand{\gpath}{\text{path}}
\newcommand{\layer}{\text{layer}}
\newcommand{\DAG}{\text{DAG}}
\newcommand{\dout}{d_\text{out}}
\newcommand{\din}{d_\text{in}}
\newcommand{\IN}{\text{in}}
\newcommand{\OUT}{\text{out}}
\newtheorem{claim}{Claim}
\newenvironment{sketch}{\paragraph{Proof sketch}\mbox{}}{\hfill\BlackBox}
\newcommand{\vin}{v_{in}}
\newcommand{\vout}{v_{out}}
\newcommand{\relu}{\sigma_{\textsc{relu}}}
\newcommand{\gnorm}[1]{\norm{#1}}
\newcommand{\mugeom}{\mu^{\textit{geo}}}
\newcommand{\pathr}{\phi}
\newcommand{\convexnn}{\nu}
\newcommand{\RSGD}{Path-SGD }



% \mode<handout>
% %\mode<presentation>
{
\usetheme{default}
}
\setbeamertemplate{footline}[frame number]

\bibliographystyle{alpha}

\title{Path-SGD: Path-Normalized Optimization in Deep Neural Networks}

\date{\textcolor{blue}{\today}}
\author{Ramchandran Muthukumar}

\begin{document}

\begin{frame}
\titlepage
\end{frame}


\begin{frame}{Quiz}
\bit 
\item Which of these algorithms perform better for balanced network as opposed to unbalanced network 
\begin{enumerate}
	\item Gradient descent
	\item Stochastic gradient descent
	\item AdaGrad
	\item \RSGD
\end{enumerate}

\item Modifying weights of RELU Feedforward Neural Network \_\_\_\_\_ changes the predictions.  
\begin{enumerate}
	\item always 
	\item sometimes
	\item never 
\end{enumerate}
\eit 	
\end{frame}



\section{Geometry}

\begin{frame}{Motivation - Link Between Geometry and Optimization Algorithms}
\bit 
\item Geometry = measure of distance, norm or divergence. 

\item Some hoices are $\ell_1$ and $\ell_2$ norms. 

\item Optimization algorithms are tied to geometry inherently. 

\item Gradient descent = steepest descent w.r.t. $\ell_2$ norm. 

\item Coordinate descent = steepest descent w.r.t.  $\ell_1$ norm. 

\eit 
\end{frame}

\begin{frame}{Impact of geometry on learning}
\bit 

\item The choice of geometry $\Rightarrow$ a choice of regularization on weights. 

\item Ideal properties of a geometry -
\begin{enumerate}
	\item faster optimization
	\item better implicit regularization 
\end{enumerate}
\item Is $\ell_2$ geometry the best choice for learning deep neural networks?
\eit 
\end{frame}

\begin{frame}{Notations}

\begin{definition}[RELU Feed-forward Neural Network]
A feed-forward neural network computes a function $f:\mathbb{R}^D \rightarrow \mathbb{R}^C$.

It is defined by a directed acyclic graph $G(V,E)$ with 
\begin{enumerate} 
	\item $D$ input nodes
	$\vin[1],\dots,\vin[D]\in V$
	\item $C$ output nodes $\vout[1],\dots,
	\vout[C]\in V$.
\end{enumerate}

For two vertices $u_1,u_2 \in V$, the weight on the corresponding edge $u_1\rightarrow u_2$ is denoted as $w_{(u_1\rightarrow u_2)} \in \mathbb{R}$. 

The network is parameterized by the collection of edge weights $\mathbf{w}$.

The activation function
$\relu(x) = \max\{0,x\}$ acts on the internal nodes (hidden
units).

We denote the function computed by this network as
$f_{G,\mathbf{w},\relu}$. 
\end{definition}

\end{frame}


\begin{frame}{RELU Networks are Rescaling Invariant}

\begin{figure}
	\includegraphics[scale=0.2]{neural_net.jpeg}
\end{figure}


\bit 
\item Consider any hidden unit $v$.

\item Scale down the incoming edges to $v$ by a factor $c$

\item Scale up the outgoing edges from $v$ by the same factor $c$.

\item The resulting network still computes the same function. 

\item Therefore RELU Networks are invariant to such a rescaling. 

\eit
\textit{What kind of activation functions would be similarly rescaling invariant?} 
 
\end{frame} 

\begin{frame}{Mathematical description of rescaling}
\begin{lemma}
RELU activation is non-negative homogeneous. For any scalar $c \geq 0$ and any $x \in \mathbb{R}$, $\relu(cx) = c\relu(x)$. 
\end{lemma}

\begin{definition}[Rescaling Function]
For any node $v \in V$, a scaling factor $c > 0$ and the weights of the network $\mathbf{w}$ , We define the {\em rescaling function} $\rho_{c,v}(\cdot) : w \mapsto \tilde{w}$ such that for all edges $(u_1\rightarrow u_2)\in E$,
\begin{equation}
\tilde{w}_{(u_1\rightarrow u_2)}=
\begin{cases}
cw_{(u_1\rightarrow u_2)}& u_2=v,\\
\frac{1}{c}w_{(u_1\rightarrow u_2)}& u_1=v,\\
w_{(u_1\rightarrow u_2)}& \text{otherwise.}\\
\end{cases}
\end{equation}
\end{definition}

A network rescaled with $\rho_{c,v}(\cdot)$ computes the same function, i.e. 
$$f_{G,\mathbf{w},\relu} = f_{G,\rho_{c,v}(\mathbf{w}),\relu}$$.

\end{frame}

\begin{frame}{Rescaling Invariance}
\begin{definition}[Rescaling Equivalent Networks]
Two RELU feedforward neural networks defined on the graph $G(V,E)$ with weights $w$ and $\tilde{w}$ are {\em rescaling equivalent} if and only if one of them can be transformed to another by applying a sequence of rescaling functions $\rho_{c,v}$.
	
We denote this property by $w\sim \tilde{w}$
\end{definition}
\end{frame}

\begin{frame}{Rescaling Invariance}
\begin{definition}[Rescaling Invariant Optimization Method]
An optimization method is {\em rescaling invariant} if its updates on rescaling equivalent networks are rescaling equivalent.

Let the initial weight matrices from rescaling equivalent  networks are $\tilde{w}^{(0)}\sim w^{(0)}$. 

For all iterations $t$ under the optimization method, the weight matrices remain rescaling equivalent and we have $\tilde{w}^{(t)} \sim
w^{(t)}$.	
\end{definition}
\end{frame}

\begin{frame}{Rescaling and Unbalanced Networks}
\begin{definition}[Balanced Networks]
We say that a network is {\em balanced} if the norm of
incoming weights to different units are roughly the same or within a
small range. 
\end{definition}

Balanced networks can be rescaled to equivalent unbalanced networks. 

Gradient descent and SGD perform very poorly on ``unbalanced''
networks.  

\begin{figure}
	\includegraphics[scale=0.2]{mnist-compare.pdf}
	\caption{Comparison of SGD on equivalent balanced and unbalanced networks for training on MNIST}
\end{figure}

\end{frame}

\begin{frame}{Issues with Unbalanced Networks}

\bit
\item Gradient descent is \emph{not} rescaling invariant. 
\item Scaling down the weights of an edge will scale up the gradient. 
\item Larger weights remain almost unchanged 
\item Smaller weights blow up

\begin{figure}
	\includegraphics[scale=0.3]{pic3.pdf}
	%\caption{}
\end{figure}

\eit
\textit{ Can you think of a way to fix this?
}
\end{frame}


\begin{frame}{Possible Fix?}
\bit 
\item We can rescale the weights after each update to a more balanced network.
\eit 
\end{frame} 

\begin{frame}{Additional Issues with Unbalanced Network}
\bit 
\item Very different relative changes in weights compared to a balanced network. 

\begin{figure}
	\includegraphics[scale=0.24]{pic4.pdf}
\end{figure}
\item After just a single update two rescaling equivalent networks could end up computing very different functions
\eit
\end{frame}

\begin{frame}{Rescaling Invariant}

\bit 
\item Predictions (or function computed) by RELU Feed-forward neural networks are rescaling invariant.
\item We want a geometry and a corresponding optimization method that is also rescaling invariant 

\item This paper considers a rescaling invariant geometry inspired by max-norm regularization.

\eit 

\end{frame}


\begin{frame}{Scale measures for deep networks}
\begin{definition}[Group-Norm Regularizer]
The generic group-norm regularizer
parametrized by $1\leq p,q \leq\infty$ is defined as
\begin{equation*}
\label{eq:mu}
\mu_{p,q}(w) = \left(\sum_{v \in V}\left(\sum_{(u\rightarrow v) \in E} \left\lvert w_{(u\rightarrow v)}\right\rvert ^p\right)^{q/p}\right)^{1/q}.
\end{equation*} 
\end{definition}

\bit
\item When $p=q=1$ this is $\ell_1$ regularization. 
\item When $p=q=2$ this is weight decay (most commonly used). 
\eit 

\begin{definition}[Max-norm regularizer]
The max-norm regularizer is defined as 
\begin{equation*}
\mu_{p,\infty}(w) =\sup_{v \in V}\left(\sum_{(u\rightarrow v) \in E} \left\lvert w_{(u\rightarrow v)}\right\rvert ^p\right)^{1/p}
\end{equation*}
This is equivalent to setting $q=\infty$ in the per-unit regularizer. 
\end{definition}
\end{frame}

\begin{frame}{Issues with Max-norm regularizer}

\bit
\item Max-norm regularization is
extreme because the value of regularizer corresponds to
the highest value among all nodes.

\item But it has empirically been shown to be effective for RELU networks. 

\item This could be because of RELU networks can be rebalanced such that all hidden units have the similar norm. 

\eit 
\end{frame} 

\begin{frame}{Issues with Max-norm regularizer}
\bit 
\item The max-norm regularizer $\mu_{p,\infty}$ is also \textbf{not} rescaling invariant.

\item We want a rescaling-invariant regularizer. 

\item We can instead look for the minimum value of the max-norm regularizer among all rescaling equivalent networks. 
\eit

\end{frame}

\begin{frame}{Paths in Feed-forward neural network}

\begin{definition}[Path vector]
Consider a RELU feed-forward
network parameterized by the graph $G(V,E)$ and edge weights $\mathbf{w}$.

Let $P_{i,j} = \{e_1,e_2,\ldots,e_d\}$ be a path from the $i$-th input unit $v_{in}[i]$ to the $j$-th output unit $v_{out}[j]$. Let $|P|$ be the number of such unique paths. 

The {\em path vector} $\pi(\textbf{w}) \in \mathbb{R}^{|P|}$ is defined as the vector where each co-ordinate is the product of all weights along a path. 
 
$$\displaystyle \pi_{P_{i,j}}(\mathbf{w}) = \prod_{k=1}^d \mathbf{w}_{e_k}$$
\end{definition}
\end{frame}

\begin{frame}{$\ell_p$-path regularizer}
\begin{definition}[$\ell_p$-path regularizer]
The $\ell_p$-path regularizer $\phi_p(\mathbf{w})$ is defined as the $\ell_p$ norm of
$\pi(\textbf{w})$:
\begin{equation}\label{eq:defphi}
\phi_p(\textbf{w}) = \norm{\pi(\textbf{w})}_p = 
\left(\sum_{\vin[i] \overset{e_1}\rightarrow v_1\overset{e_2}\rightarrow v_2\dots\overset{e_d}{\rightarrow}\vout[j]} \left|\prod_{k=1}^d \textbf{w}_{e_k}\right|^p\right)^{1/p}
\end{equation}

\end{definition}

\textit{What would be the complexity of computing $\phi_p(\mathbf{w})$ ? 
}
\end{frame}

\begin{frame}{Computation of $\ell_p$-path regularizer}
\bit 
\item The $\ell_p$-path regularizer can be computed
efficiently in a \emph{single forward step} by the following recursive definition 

\begin{align*}
\phi_{p,v}(\mathbf{w}) &= \sum_{(u\rightarrow v)\in E} \phi_{p,u}(\mathbf{w}) \cdot (\mathbf{w}_{u \rightarrow v})^p\ \\
\phi_{p}(\mathbf{w}) & = \sum_{u\in V_{out}} \phi_{p,u}(\mathbf{w})
\end{align*}
\eit 

\end{frame} 

\begin{frame}{Link between $\ell_p$-path regularizer and max-norm regularizer}
\bit 

\begin{lemma}
	The $\ell_p$-path regularizer and per-unit regularizer satisfy the following relation :
	$$\displaystyle \phi_p(\mathbf{w}) = \min_{\tilde{\mathbf{w}} \sim \mathbf{w}} \bigg(\mu_{p,\infty}(\tilde{\mathbf{w}})\bigg)^d$$
		
	where $d$ is the depth of the neural network. 
\end{lemma}

\item Hence the $\ell_p$ path-regularizer $\phi_p$ is invariant to rescaling. 

 For any $\tilde{\mathbf{w}} \sim \mathbf{w}$, $\phi_p(\tilde{\mathbf{w}})=\phi_p(\mathbf{w})$
\eit 
\end{frame}

\section{Path-SGD: Approximate Path-Regularized Steepest Descent}
\begin{frame}{Steepest Descent w.r.t. $\ell_p$-path regularizer}
\bit
\item The steepest descent update is given by the solution to the optimization problem 
\begin{align*}
\textbf{w}^{(t+1)} :=&\arg\min_{\textbf{w}} \;\;\eta \nabla L(\textbf{w}^{(t)}) (\textbf{w}-\textbf{w}^{(t)}) + \frac{1}{2}\norm{\pi(\textbf{w})-\pi(\textbf{w}^{(t)})}_p^2\\ 
 = &\arg\min_{\textbf{w}} \; \; J^{(t)}(\textbf{w})
\end{align*}

\item Here the path regularizer term is 
\begin{equation*}
\resizebox{\hsize}{!}{%
$\displaystyle \norm{\pi(\textbf{w})-\pi(\textbf{w}^{(t)})}_p^2 = \left(\sum_{\vin[i] \overset{e_1}\rightarrow v_1\overset{e_2}\rightarrow v_2\dots\overset{e_d}{\rightarrow}\vout[j]}\left(\prod_{k=1}^d \textbf{w}_{e_k} - \prod_{k=1}^d \textbf{w}^{(t)}_{e_k})\right)^p\right)^{2/p}$
}
\end{equation*}
\eit 
\end{frame}

\begin{frame}{Steepest descent w.r.t. $\ell_p$-path regularizer}
\bit 
\item The steepest descent step is hard to calculate exactly.  

\item Instead, we will update each coordinate $\textbf{w}_e$ independently (and synchronously)  
\begin{equation}
\textbf{w}_e^{(t+1)} =\arg\min_{\textbf{w}_e} \;J^{(t)}(\textbf{w}) \qquad \text{s.t.}\;\;\forall_{e'\neq e} \;\; \textbf{w}_{e'}=\textbf{w}^{(t)}_{e'}
\end{equation}

\item Taking the partial derivative with respect to $\textbf{w}_e$ and setting it to zero we obtain:
\begin{equation*}
0 =\eta \frac{\partial L}{\partial \textbf{w}_e}(\textbf{w}^{(t)}) - \left(\textbf{w}_e-\textbf{w}^{(t)}_e\right) 
\left(\sum_{v_{\text{in}}[i] \dots \stackrel{e}{\rightarrow} \dots v_{\text{out}}[j]} \prod_{e_k\neq e} |{\textbf{w}^{(t)}_e}^p|\right)^{2/p}
\end{equation*}
\item $v_{\text{in}}[i] \dots \stackrel{e}{\rightarrow} \dots v_{\text{out}}[j]$ denotes the paths from the  $i$-th input unit to the $j$-th output unit that includes edge $e$. 
\eit 
\end{frame}

\begin{frame}{Path-SGD}
\bit 
\item Solving for $\textbf{w}_e$ gives the update rule:
\begin{equation*}
\hat{\textbf{w}}^{(t+1)}_e = \textbf{w}^{(t)}_e- \frac{\eta}{\gamma_p(\textbf{w}^{(t)},e)}\frac{\partial L}{\partial \textbf{w}}(\textbf{w}^{(t)})
\end{equation*}

\item Here $\gamma_p(\textbf{w},e)$ is defined as 
\begin{equation*}
\gamma_p(\textbf{w},e) =\left(\sum_{v_{\text{in}}[i] \dots \stackrel{e}{\rightarrow} \dots v_{\text{out}}[j]} \prod_{e_k\neq e} |{\textbf{w}_{e_k}}|^p\right)^{2/p}
\end{equation*}

\item The optimization algorithms with the above update rule is called \emph{path-normalized gradient descent}. 

\item In stochastic settings, we refer to it as \emph{\RSGD}.
\eit 
\end{frame}

\begin{frame}{\RSGD}
\begin{theorem}
	\RSGD is rescaling invariant, i.e. for any $c>0$ and $v\in E$, $$\tilde{\textbf{w}}^{(t)} = \rho_{c,v}(\textbf{w}^{(t)}) \quad \Rightarrow \quad \tilde{\textbf{w}}^{(t+1)} = \rho_{c,v}(\textbf{w}^{(t+1)})$$ 
\end{theorem}
\bit 

\item If edge $e$ is neither incoming nor outgoing edge of the node $v$, then $\tilde{\textbf{w}}(e)=\textbf{w}(e)$ and $\tilde{\textbf{w}}^{(t+1)}_e=\textbf{w}^{(t+1)}_e$

\item If edge $e$ is an incoming edge to $v$, then $\tilde{\textbf{w}}^{(t)}(e)=c\textbf{w}^{(t)}(e)$.

\item In this case, outgoing edges of $v$ are divided by $c$. Therefore 
\begin{equation*}
\gamma_p(\tilde{\textbf{w}}^{(t)},e) = \frac{\gamma_p(\textbf{w}^{(t)},e)}{c^2}, \quad  \frac{\partial L}{\partial \textbf{w}_e}(\tilde{\textbf{w}}^{(t)})= \frac{\partial L}{c\partial \textbf{w}_e}(\textbf{w}^{(t)})
\end{equation*} 

\eit 
\end{frame} 

\begin{frame}{\RSGD}
\bit 
\item The \RSGD update on $\tilde{\mathbf{w}}$ is 
\begin{align*}
{\tilde{\textbf{w}}}^{(t+1)}_e &= c\textbf{w}^{(t)}_e - \frac{c^2\eta}{\gamma_p(\textbf{w}^{(t)},e)} \cdot \frac{\partial L}{c\partial \textbf{w}_e}(\textbf{w}^{(t)})\\
&= c\left(\textbf{w}^{(t)} - \frac{\eta}{\gamma_p(\textbf{w}^{(t)},e)} \cdot \frac{\partial L}{\partial \textbf{w}_e}(\textbf{w}^{(t)})\right) = c\textbf{w}^{(t+1)}_e.
\end{align*}

\item A similar argument follows when $e$ is an outgoing edge of node $v$. 

\item Therefore, \RSGD is rescaling invariant.
\eit 
\end{frame}

\section{Experiments} 

\begin{frame}{Computational Setup}

\bit 
\item Feed-forward networks with 2 layers of 4000 hidden units each. 
\item Minibatches of size 100, step size $10^{-\alpha}$ for integer $\alpha <10$.
\item Initialization for edge weights
\begin{enumerate}
	\item Balanced : weights drawn i.i.d from Gaussain Distribution. 
	\item Unbalanced : Choose balanced weights and rescale 2000 hidden units by scale factor c. 
\end{enumerate}  
\item Dropout with $0.5$ probability of retaining each unit. 
\item Dropout experiments only on balanced networks. 
\eit 

\end{frame}

\begin{frame}{Experiments without Dropout : CIFAR-10 and MNIST Dataset}
	\centering
	\begin{tabular}{ccc}
		\includegraphics[width=0.33\linewidth]{cifar10-surr.pdf} &
		\includegraphics[width=0.33\linewidth]{cifar10-train.pdf} &
		\includegraphics[width=0.33\linewidth]{cifar10-test.pdf} \\
		\includegraphics[width=0.33\linewidth]{mnist-surr.pdf} &
		\includegraphics[width=0.33\linewidth]{mnist-train.pdf} &
		\includegraphics[width=0.33\linewidth]{mnist-test.pdf} \\
		\scriptsize epochs & \scriptsize epochs & \scriptsize epochs \\
		& & \\ 
		Cross Entropy Loss & Training Error & Testing Error
	\end{tabular}
	\begin{picture}(0,-100)(150,-10)
	\rotatebox{90}{\put(120, 400){CIFAR-10}\put(50, 400){MNIST}}
	\end{picture}
\end{frame}

\begin{frame}{Discussion for no-dropout case}
	\bit 
	\item Learning curves of \RSGD on balanced and unbalanced initializations were nearly identical.  
	
	\item Unbalanced initialization drastically affects SGD and AdaGrad. 
	
	\item \RSGD converges faster than SGD and AdaGrad. 
	
	\item \RSGD also has lesser testing error (better implicit regularization?). 
	
	\eit 
\end{frame}

\begin{frame}{Experiments with Dropout: CIFAR-10 and MNIST Dataset}
	\centering
	\begin{tabular}{ccc}
		\includegraphics[width=0.33\linewidth]{drop-cifar10-surr.pdf} &
		\includegraphics[width=0.33\linewidth]{drop-cifar10-train.pdf} &
		\includegraphics[width=0.33\linewidth]{drop-cifar10-test.pdf} \\
		\includegraphics[width=0.33\linewidth]{drop-mnist-surr.pdf} &
		\includegraphics[width=0.33\linewidth]{drop-mnist-train.pdf} &
		\includegraphics[width=0.33\linewidth]{drop-mnist-test.pdf} \\
	\scriptsize epochs & \scriptsize epochs & \scriptsize epochs \\
	& & \\ 
	Cross Entropy Loss & Training Error & Testing Error
\end{tabular}
\begin{picture}(0,-100)(150,-10)
\rotatebox{90}{\put(120, 400){CIFAR-10}\put(50, 400){MNIST}}
\end{picture}
\par
\end{frame}

\begin{frame}{Discussion for dropout case}
\bit 
\item \RSGD is atleast as fast as SGD and AdaGrad. (Much faster for CIFAR-10).  
\item \RSGD again has lesser testing error. 
\item Thus empirically, \RSGD achieves the same accuracy faster and generalizes better. 
\eit 
\end{frame}

\section{Conclusion}

\begin{frame}{Conclusion : Effects of Geometry}
\bit 
\item Choosing alternative geometry can result in faster optimization and better generalization. 
\item Rescaling Invariance is an appropriate property for RELU networks. 
\item \RSGD empirically performs better even for unbalanced networks. 
\item Combining \RSGD with momentum or other heuresitcs might improve results.
\eit 	
\end{frame}
\end{document}